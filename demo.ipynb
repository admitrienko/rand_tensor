{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# imports\n",
    "\n",
    "from itertools import chain\n",
    "import pylab as pb\n",
    "import GPy as gpy\n",
    "import matplotlib\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline  \n",
    "%config InlineBackend.figure_format = 'svg'\n",
    "matplotlib.rcParams['figure.figsize'] = (8,5)\n",
    "\n",
    "import numpy as np\n",
    "from numpy import random as nprnd\n",
    "from scipy import linalg as sciLA\n",
    "from scipy import optimize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Specify covariance constraints for a tensor of dimensionality 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# specify tensor\n",
    "nmodes = 6\n",
    "sizes = nprnd.randint(10, size = nmodes)+1\n",
    "# specify covariance constraints\n",
    "# genarate marginal covariance matrices of size specified in dim\n",
    "covConst = []\n",
    "eigenValuesTruelist = []\n",
    "eigenVectorsTruelist = []\n",
    "scale = nprnd.randint(10)+1\n",
    "for i in range(nmodes):\n",
    "    eigVectors = sciLA.orth(nprnd.randn(sizes[i], sizes[i]))\n",
    "    eigValues = abs(nprnd.randn(sizes[i]))\n",
    "    eigValues = scale*(eigValues/np.sum(eigValues))\n",
    "    Sigma = np.dot(np.dot(eigVectors, np.diag(eigValues)), eigVectors.T)\n",
    "    covConst.append(Sigma)\n",
    "    eigenValuesTruelist.append(eigValues)\n",
    "    eigenVectorsTruelist.append(eigVectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def kron_mvprod(As, b):\n",
    "    x = b\n",
    "    numDraws = b[0, :].size\n",
    "    CTN = b[:, 0].size\n",
    "    for d in range(len(As)):\n",
    "        A = As[d]\n",
    "        Gd = A[:,0].size\n",
    "        X = np.reshape(x, tuple([Gd, CTN*numDraws/Gd]), order = 'F')\n",
    "        Z = np.dot(A, X).T\n",
    "        x = np.reshape(Z, tuple([CTN,numDraws]), order = 'F')\n",
    "    x = np.reshape(x, tuple([CTN*numDraws, 1]), order = 'F')\n",
    "    x = np.reshape(x, tuple([numDraws, CTN]), order = 'F').T\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def diagKronSum(Ds):\n",
    "    nmodes = len(Ds)\n",
    "    kronSumDs = np.zeros(1)\n",
    "    for i in range(nmodes):\n",
    "        kronSumDs = np.add(np.outer(kronSumDs, np.ones(len(Ds[i]))),np.outer(np.ones(len(kronSumDs)),Ds[i]))\n",
    "        kronSumDs = np.hstack(kronSumDs.T)\n",
    "    return kronSumDs\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
    "#% <tensorMaxEntropy>\n",
    "#% Copyright (C) 2016 Gamaleldin F. Elsayed and John P. Cunningham \n",
    "#%       (see full notice in README)\n",
    "#%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
    "#% sumA = sumTensor(A, sumDim)\n",
    "#%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
    "#% This function evaluates the sum of tensor at specific dimensions\n",
    "#%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
    "#% Inputs:\n",
    "#%       - A: is the input n-dimesnional tensor\n",
    "#%       - sumDim: the dimensions to sum over.\n",
    "#% Outputs:\n",
    "#%       - sumA: an n-dimensional tensor of the sum of tensor A at the \n",
    "#%       specified dimensions. The dimensions specified by sumDim will be of\n",
    "#%       size 1.\n",
    "#%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
    "#%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
    "\n",
    "def sumTensor(T, sumDim):\n",
    "    sumT = T\n",
    "    for i in range(len(sumDim)):\n",
    "        sumT = np.expand_dims(np.sum(sumT, sumDim[i]), axis = sumDim[i])\n",
    "    return sumT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def logObjectiveMaxEntropyTensor(logL, params):\n",
    "    Lagrangians = []\n",
    "    for x in params.tensorIxs:\n",
    "        Lagrangians.append(np.exp(logL[np.add(sum(params.sizes[0:x]),range(int(params.sizes[x])))]))\n",
    "############ building blocks of the gradient ###########\n",
    "    LsTensor = diagKronSum(Lagrangians)\n",
    "    LsTensor = np.reshape(LsTensor, tuple(params.sizes), order='F')                                 # kronecker sum of the lagrangian matrices eigenvalues\n",
    "    invLsTensor = 1.0/LsTensor                                            # elementwise inverse of the above\n",
    "    invSquareLsTensor = 1.0/LsTensor**2                                     # elementwise inverse square of the above\n",
    "    Er = []\n",
    "    logSums = []\n",
    "    fx = []                                             # the log transformed cost decomposed to different tensor dimensions\n",
    "    gradf_logL = np.zeros(len(logL))\n",
    "    for x in params.tensorIxs:\n",
    "        nxSet = list(set(params.tensorIxs).difference(set([x])))\n",
    "        logSums.append(np.log(sumTensor(invLsTensor, nxSet)))                    # elementwise log of invLsTensor)\n",
    "        Er.append(np.reshape(params.logEigValues[x], logSums[x].shape, order='F')- logSums[x])     # error with respect to each marginal covariance eigenvalue\n",
    "\n",
    "\n",
    "        fx.append(np.reshape(Er[x], tuple([sizes[x], 1]), order='F')**2)                               \n",
    "    f = sum(np.vstack(fx))/params.normalizeTerm\n",
    "\n",
    "\n",
    "############ build the gradient from the blocks ###########\n",
    "    for x in params.tensorIxs:\n",
    "        nxSet = list(set(params.tensorIxs).difference(set([x])))\n",
    "        gradfx_logLx = np.reshape(((2*Er[x]/sumTensor(invLsTensor, nxSet))*sumTensor(invSquareLsTensor, nxSet)),tuple([params.sizes[x],1]), order='F')*np.reshape(Lagrangians[x], tuple([params.sizes[x],1]), order='F')\n",
    "        gradf_logLx = gradfx_logLx\n",
    "        for y in nxSet:\n",
    "            nySet = list(set(params.tensorIxs).difference(set([y])))\n",
    "            nxySet = list(set(params.tensorIxs).difference(set([x, y])))\n",
    "            gradfy_logLx = np.reshape(sumTensor((2*Er[y]/sumTensor(invLsTensor, nySet))*sumTensor(invSquareLsTensor, nxySet), [y]), tuple([params.sizes[x] ,1]), order='F')*np.reshape(Lagrangians[x], tuple([params.sizes[x] ,1]), order='F')\n",
    "            gradf_logLx = gradf_logLx+gradfy_logLx\n",
    "\n",
    "        gradf_logL[np.add(sum(params.sizes[0:x]),range(int(params.sizes[x])))] = gradf_logLx\n",
    "    \n",
    "    gradf_logL = gradf_logL/params.normalizeTerm  \n",
    "############ return ###########\n",
    "    return f, gradf_logL\n",
    "\n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Gamaleldin Elsayed\n",
    "\n",
    "# translated to python based on Carl Edward Rasmussen minimize.m function\n",
    "# X is intial starting point\n",
    "# obj_Fn is the function that takes (X, params) as inputs and spits back the current value of the objective and the gradient.\n",
    "# length is the maximum allowed number of iterations.\n",
    "# params any variables other than X for the obj_Fn to compute the cost value and the gradient.\n",
    "\n",
    "def minimize(X, obj_Fn, length, params):\n",
    "    #% imports:\n",
    "    import numpy as np;\n",
    "    \n",
    "    supressOutPut = True;\n",
    "    INT = 0.1;    # don't reevaluate within 0.1 of the limit of the current bracket\n",
    "    EXT = 3.0;    # extrapolate maximum 3 times the current step-size\n",
    "    MAX = 20;     # max 20 function evaluations per line search\n",
    "    RATIO = 10.0;   # maximum allowed slope ratio\n",
    "    SIG = 0.1; \n",
    "    RHO = SIG/2.0; # SIG and RHO are the constants controlling the Wolfe-\n",
    "    # Powell conditions. SIG is the maximum allowed absolute ratio between\n",
    "    # previous and new slopes (derivatives in the search direction), thus setting\n",
    "    # SIG to low (positive) values forces higher precision in the line-searches.\n",
    "    # RHO is the minimum allowed fraction of the expected (from the slope at the\n",
    "    # initial point in the linesearch). Constants must satisfy 0 < RHO < SIG < 1.\n",
    "    # Tuning of SIG (depending on the nature of the function to be optimized) may\n",
    "    # speed up the minimization; it is probably not worth playing much with RHO.\n",
    "    realmin = np.finfo(np.double).tiny \n",
    "    \n",
    "    def unwrap(s):\n",
    "        l = s.shape\n",
    "        v = np.reshape(s, np.prod(l), order = 'F')\n",
    "        return v\n",
    "        \n",
    "    def rewrap(s, v):\n",
    "        l = s.shape;\n",
    "        v = np.reshape(v, l, order = 'F')\n",
    "        return v\n",
    "        \n",
    "\n",
    "    S='Linesearch'\n",
    "\n",
    "\n",
    "    i = 0                                            # zero the run length counter\n",
    "    ls_failed = False                             # no previous line search has failed\n",
    "    f0, df0 = obj_Fn(X, params)    # get function value and gradient\n",
    "    Z = X;X = unwrap(X); df0 = unwrap(df0);\n",
    "    if not supressOutPut:\n",
    "        print \"%s %6i;  Value %4.6e\\r\" %(S, i, f0)\n",
    "    fX = []\n",
    "    fX.append(f0);\n",
    "\n",
    "    s = -df0; \n",
    "    d0 = -np.dot(s.T,s)           # initial search direction (steepest) and slope\n",
    "    x3 = 1.0/(1.0-d0)\n",
    "\n",
    "    while i < length:              # while not finished\n",
    "        i = i + 1                   # count iterations\n",
    "        X0 = X; F0 = f0; dF0 = df0; # make a copy of current values\n",
    "        M = MAX; \n",
    "\n",
    "\n",
    "        while True:   # keep extrapolating as long as necessary\n",
    "            x2 = 0; f2 = f0; d2 = d0; f3 = f0; df3 = df0;\n",
    "            success = False;\n",
    "\n",
    "            while (not success) and (M > 0):\n",
    "                try:\n",
    "                    M = M - 1.0; i = i + 1;          # count epochs\n",
    "                    f3, df3 = obj_Fn(rewrap(Z, X+x3*s), params);\n",
    "                    df3 = unwrap(df3);\n",
    "                    if np.isnan(f3) or np.isinf(f3) or any(np.isnan(df3)+np.isinf(df3)):\n",
    "                        print \"error(' ')\"\n",
    "                    success = True;\n",
    "                except:         # catch any error which occured in f\n",
    "                    x3 = (x2+x3)/2;  # bisect and try again\n",
    "\n",
    "\n",
    "            if f3 < F0:\n",
    "                X0 = X+x3*s; F0 = f3; dF0 = df3;       # keep best values\n",
    "            d3 = np.dot(df3.T,s);                     # new slope\n",
    "\n",
    "\n",
    "            if (d3 > (SIG*d0)) or (f3 > (f0+x3*RHO*d0)) or (M == 0):  # are we done extrapolating?\n",
    "                break;\n",
    "\n",
    "\n",
    "            x1 = x2; f1 = f2; d1 = d2;                      # move point 2 to point 1\n",
    "            x2 = x3; f2 = f3; d2 = d3;                      # move point 3 to point 2\n",
    "            A = 6*(f1-f2)+3*(d2+d1)*(x2-x1);                # make cubic extrapolation\n",
    "            B = 3*(f2-f1)-(2*d1+d2)*(x2-x1);\n",
    "\n",
    "            x3 = x1-d1*(x2-x1)**2/(B+np.sqrt(B*B-A*d1*(x2-x1))); # num. error possible, ok!\n",
    "            if (not np.isreal(x3)) or np.isnan(x3) or np.isinf(x3) or (x3 < 0.): # num prob | wrong sign?\n",
    "                x3 = x2*EXT;                    # extrapolate maximum amount\n",
    "            elif x3 > x2*EXT:                  # new point beyond extrapolation limit?\n",
    "                x3 = x2*EXT;                    # extrapolate maximum amount\n",
    "            elif x3 < (x2+INT*(x2-x1)):         # new point too close to previous point?\n",
    "                x3 = x2+INT*(x2-x1);\n",
    "                                   ### end extrapolation ####\n",
    "\n",
    "        while (abs(d3) > -SIG*d0 or (f3 > (f0+x3*RHO*d0))) and (M > 0):  # keep interpolating\n",
    "            if d3 > 0 or f3 > f0+x3*RHO*d0:             # choose subinterval\n",
    "                x4 = x3; f4 = f3; d4 = d3;               # move point 3 to point 4\n",
    "            else:\n",
    "                x2 = x3; f2 = f3; d2 = d3;               # move point 3 to point 2\n",
    "\n",
    "            if f4 > f0:          \n",
    "                x3 = x2-(0.5*d2*(x4-x2)**2)/(f4-f2-d2*(x4-x2)); # quadratic interpolation\n",
    "            else:\n",
    "                A = 6*(f2-f4)/(x4-x2)+3*(d4+d2);          # cubic interpolation\n",
    "                B = 3*(f4-f2)-(2*d2+d4)*(x4-x2);\n",
    "                x3 = x2+(np.sqrt(B*B-A*d2*(x4-x2)**2)-B)/A;   # num. error possible, ok!\n",
    "\n",
    "            if np.isnan(x3) or np.isinf(x3):\n",
    "                x3 = (x2+x4)/2;       # if we had a numerical problem then bisect\n",
    "\n",
    "\n",
    "            x3 = max(min(x3, x4-INT*(x4-x2)),x2+INT*(x4-x2));  # don't accept too close\n",
    "            f3, df3 = obj_Fn(rewrap(Z, X+x3*s), params);\n",
    "            df3 = unwrap(df3);\n",
    "\n",
    "            if f3 < F0:\n",
    "                X0 = X+x3*s; F0 = f3; dF0 = df3;    # keep best values\n",
    "            M = M - 1.0; i = i + 1;               # count epochs\n",
    "            d3 = np.dot(df3.T,s);               # new slope\n",
    "                               #### end interpolation ####\n",
    "\n",
    "        if (abs(d3) < (-SIG*d0)) and (f3 < (f0+x3*RHO*d0)): # if line search succeeded\n",
    "            X = X+x3*s; f0 = f3; fX.append(f0);              # update variables\n",
    "            if not supressOutPut:\n",
    "                print \"%s %6i;  Value %4.6e\\r\" %(S, i, f0);\n",
    "            s = np.dot((np.dot(df3.T, df3)-np.dot(df0.T,df3))/(np.dot(df0.T,df0)), s) - df3;  # Polack-Ribiere CG direction\n",
    "            df0 = df3;                        # swap derivatives\n",
    "            d3 = d0; d0 = np.dot(df0.T,s);\n",
    "            if d0 > 0:                         # new slope must be negative\n",
    "                s = -df0; d0 = -np.dot(s.T,s); # otherwise use steepest direction\n",
    "\n",
    "            x3 = x3 * min(RATIO, d3/(d0-realmin));  # slope ratio but max RATIO\n",
    "            ls_failed = False;                      # this line search did not fail\n",
    "        else:\n",
    "            X = X0; f0 = F0; df0 = dF0;            # restore best point so far\n",
    "            if ls_failed or (i > abs(length)):      # line search failed twice in a row\n",
    "                break;                             # or we ran out of time, so we give up\n",
    "\n",
    "            s = -df0; d0 = -np.dot(s.T,s);           # try steepest\n",
    "            x3 = 1.0/(1.0-d0);                     \n",
    "            ls_failed = True;                        # this line search failed\n",
    "    X = rewrap(Z,X);\n",
    "    return X, fX, i\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def optMaxEntropy(eigValues, maxIter, figFlg):\n",
    "#  if the marginal covariances are low rank then the number of variables \n",
    "#  that we solve for are less. If full rank the number of variables that we \n",
    "#  solve for are equal to the sum of the tensor dimensions.\n",
    "############# define optimization params class #####################\n",
    "    class optParams:\n",
    "        logEigValues = []\n",
    "        nmodes = []\n",
    "        tensorIxs = []\n",
    "        sizes = []\n",
    "        normalizeTerm = []\n",
    "        logL = []\n",
    "########################## initializations #########################\n",
    "    params = optParams()\n",
    "#     figFlg = True          # display summary figure flag\n",
    "    params.nmodes = len(eigValues);              # tensor size; i.e. the number of different dimensions of the tensor\n",
    "    sizes = np.zeros(params.nmodes)              # tensor dimensions\n",
    "    params.tensorIxs = range(params.nmodes)  \n",
    "    threshold = -10                                     # if an eigenvalue is below this threshold it is considered 0. \n",
    "    for x in params.tensorIxs:\n",
    "        sizes[x] = len(eigValues[x])\n",
    "    \n",
    "# instead of solving for the largrangians directly we optimize latent variables that is equal to the log of the lagrangians\n",
    "\n",
    "    preScale = sum(eigValues[0])/np.mean(sizes)         # prescale the eigenvalues for numerical stability\n",
    "    params.logEigValues = []                            # the log of the eigenvalues\n",
    "    params.sizes =  []                                  # true number of variables that we solve for, which is equal to the sum of the ranks of the marginal covariances                                                \n",
    "    for x in params.tensorIxs:\n",
    "        params.logEigValues.append(np.array([i for i in np.log(eigValues[x]/preScale) if i>threshold])) # eigenvalues should be order apriori\n",
    "        params.sizes.append(len(params.logEigValues[x]))\n",
    "    params.sizes = np.array(params.sizes)\n",
    "    params.normalizeTerm = sum(np.hstack(params.logEigValues)**2)\n",
    "\n",
    "####################### optimization step #############################\n",
    "# initialization of the optimization variables    \n",
    "    logL0 = []  \n",
    "    for x in params.tensorIxs:\n",
    "        nxSet = map(int, set(params.tensorIxs).difference(set([x])))\n",
    "        logL0.append(np.log(sum(params.sizes[nxSet]))-params.logEigValues[x])\n",
    "    \n",
    "#     maxIter = 1000;        # maximum allowed iterations\n",
    "    logL0 = np.array(np.hstack(logL0)).T\n",
    "    logL, f, i = minimize(logL0 ,logObjectiveMaxEntropyTensor , maxIter, params); # this function performs all the optimzation heavy lifting\n",
    "    L = np.exp(logL)\n",
    "    params.L = L\n",
    "    \n",
    "    if figFlg:\n",
    "        import matplotlib.pyplot as plt\n",
    "        plt.plot(range(len(f)), f)\n",
    "        plt.ylabel('objective fn')\n",
    "        plt.show()\n",
    "##################### convert solution from the lagrangian variables to the optimal eigenvalues of the big covariance matrix\n",
    "    Lagrangians = []                                      # save the lagrangians to the output \n",
    "    for x in params.tensorIxs: \n",
    "        Lagrangians.append(np.hstack([L[np.add(sum(params.sizes[0:x]),range(int(params.sizes[x]))).astype(int)]/preScale, float('inf')*np.ones(sizes[x]-params.sizes[x])]))\n",
    "    \n",
    "    return Lagrangians, f[-1]\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " /Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/ipykernel/__main__.py:92: RuntimeWarning:invalid value encountered in sqrt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in estimated marginal covariance of mode 0, empirically estimated from samples, is 0.21 %\n",
      "Error in estimated marginal covariance of mode 1, empirically estimated from samples, is 0.67 %\n",
      "Error in estimated marginal covariance of mode 2, empirically estimated from samples, is 0.89 %\n",
      "Error in estimated marginal covariance of mode 3, empirically estimated from samples, is 0.51 %\n",
      "Error in estimated marginal covariance of mode 4, empirically estimated from samples, is 0.93 %\n",
      "Error in estimated marginal covariance of mode 5, empirically estimated from samples, is 0.27 %\n"
     ]
    }
   ],
   "source": [
    "t = randTensors(sizes)\n",
    "t.fitMaxEntropy(covConst)\n",
    "ts = t.sampleTensors(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print ts[0].shape\n",
    "t.samplingError(ts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class randTensors:\n",
    "    def __init__(self, sizes):\n",
    "        self.sizes = sizes;\n",
    "        self.marginalCov = [];\n",
    "        self.nmodes = len(sizes);\n",
    "        self.margCovs = [];                            # marginal Covariances of the distribution\n",
    "        self.margEigVectors = [];                        # eigenvectors of the marginal Covariances of the distribution\n",
    "        self.margEigValues = [];\n",
    "        for i in range(self.nmodes):\n",
    "            self.margCovs += [np.eye(sizes[i])/sizes[i]];\n",
    "            self.margEigVectors += [np.eye(sizes[i])];\n",
    "            self.margEigValues += [np.eye(sizes[i])];\n",
    "        self.vecEigValues = np.ones(np.prod(self.sizes)); # eigenvectors of the vectorized distribution\n",
    "    \n",
    "    ###### function to check if a matrix is proper covariance matrix (i.e., symmetric posistive semidef)\n",
    "    def isproperCov(self, Sigma):  ### gamal\n",
    "        isCov = True\n",
    "        SigmaSPD= (Sigma+Sigma.T)/2  # make sure it is symmetric pos def. matrix (i.e., proper covariance matrix)\n",
    "        if not isCov:\n",
    "            print ('The covariance matrix is not correct, approximating to the nearest possible covariance')\n",
    "        return SigmaSPD;\n",
    "    \n",
    "    ###### function to compute the eigenvalues and eigenvectors of the marginal covariances\n",
    "    def margEig(self, margCovs):\n",
    "        margEigValues = [];\n",
    "        margEigVectors = [];\n",
    "        for i in range(len(margCovs)):\n",
    "            Sigma_i = margCovs[i];\n",
    "            Sigma_i = self.isproperCov(Sigma_i);\n",
    "            Q, s, _ = sciLA.svd(Sigma_i)\n",
    "            ix = np.argsort(s)[::-1]\n",
    "            s = s[ix]\n",
    "            Q = Q[:, ix]\n",
    "            margEigValues += [s];\n",
    "            margEigVectors += [Q];\n",
    "        return margEigValues, margEigVectors\n",
    "    \n",
    "    def fitMaxEntropy(self, margCovs):\n",
    "        margEigValues, margEigVectors = self.margEig(margCovs);\n",
    "        maxIter = 1000;\n",
    "        figFlg = False;\n",
    "        Lagrangians, f = optMaxEntropy(margEigValues, maxIter, figFlg);\n",
    "        if f>1e-10:\n",
    "            print (\"algorithm did not completely converged. error = %.10f \\n results may be inaccurate\" %f)\n",
    "        self.vecEigValues = 1/diagKronSum(Lagrangians);\n",
    "        self.margCovs = margCovs;\n",
    "        self.margEigValues = margEigValues;\n",
    "        self.margEigVectors = margEigVectors;\n",
    "        \n",
    "    def sampleTensors(self, nsamples):\n",
    "        vecTensors = np.zeros([np.prod(self.sizes), nsamples]);\n",
    "        for i in range(nsamples):\n",
    "            vecTensors[:, i] = nprnd.randn(np.prod(self.sizes))\n",
    "            vecTensors[:, i] = vecTensors[:, i]*np.sqrt(self.vecEigValues)\n",
    "        vecTensors = np.real(kron_mvprod(self.margEigVectors, vecTensors))\n",
    "        tensors = []\n",
    "        for i in range(nsamples):\n",
    "            tensors += [np.reshape(vecTensors[:,i], tuple(self.sizes), order = 'F')]\n",
    "        \n",
    "        _ = self.samplingError(tensors)\n",
    "        return tensors\n",
    "    \n",
    "    def empiricalMargCovs(self, tensors):\n",
    "        nsamples = len(tensors)\n",
    "        M =  self.empiricalMean(tensors)\n",
    "        sizes = np.array(tensors[0].shape)\n",
    "        nmodes = len(sizes)\n",
    "        allDim = range(nmodes)\n",
    "        estMargCov = []\n",
    "        for i in allDim:\n",
    "            estMargCov.append(np.zeros([sizes[i], sizes[i]]))\n",
    "\n",
    "        for j in range(nsamples):\n",
    "            tensor = tensors[j]-M\n",
    "            marginalCovs = []\n",
    "            for i in allDim:\n",
    "                niSet = list(set(allDim) - set([i]))\n",
    "                z = np.reshape(np.transpose(tensor, list(chain.from_iterable([[i], niSet]))), tuple([sizes[i], np.prod(sizes[niSet])]), order = 'F')\n",
    "                estMargCov[i] = (estMargCov[i]+(np.dot(z,z.T)/(nsamples-1)))\n",
    "        for i in allDim:\n",
    "            estMargCov[i] = (estMargCov[i])\n",
    "        return estMargCov\n",
    "    \n",
    "    def empiricalMean(self, tensors):\n",
    "        nsamples = len(tensors);\n",
    "        M = tensors[0]/nsamples\n",
    "        for i in range(nsamples-1):\n",
    "            M = M+tensors[i+1]/nsamples\n",
    "        return M\n",
    "    \n",
    "    def samplingError(self, tensors):\n",
    "        nsamples = len(tensors);\n",
    "        estMargCov = self.empiricalMargCovs(tensors);\n",
    "        error = [];\n",
    "        for i in range(self.nmodes):\n",
    "            error += [(sciLA.norm(estMargCov[i]-self.margCovs[i], 'fro')/sciLA.norm(self.margCovs[i], 'fro'))**2*100];\n",
    "            print \"Error in estimated marginal covariance of mode %d, empirically estimated from samples, is %.2f %%\" \\\n",
    "            %(i, error[i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def tensorNormal(covConst, numSamples):\n",
    "    nmodes = len(covConst)\n",
    "    sizes = []\n",
    "    eigenValueslist = []\n",
    "    eigenVectorslist = []\n",
    "    for i in range(nmodes):\n",
    "        Sigma_i = covConst[i]\n",
    "        Sigma_i = (Sigma_i+Sigma_i.T)/2\n",
    "        Q, s, _ = sciLA.svd(Sigma_i)\n",
    "        ix = np.argsort(s)[::-1]\n",
    "        s = s[ix]\n",
    "        Q = Q[:, ix]\n",
    "        sizes.append(len(s))\n",
    "        eigenValueslist.append(s)\n",
    "        eigenVectorslist.append(Q)\n",
    "    \n",
    "    sumEigValues = sum(eigenValueslist[0])\n",
    "\n",
    "    scaleList = []\n",
    "    for i in range(nmodes-1):\n",
    "        scaleList.append(nprnd.rand()*sumEigValues**(1/nmodes))\n",
    "    scaleList.append(sumEigValues/np.prod(scaleList))\n",
    "    \n",
    "    normalEigVectorslist = []\n",
    "    normalSqrtEigValuesList = []\n",
    "    for i in range(nmodes):\n",
    "        W = scaleList[i]/sumEigValues*covConst[i];\n",
    "        Q, s, _ = sciLA.svd(W)\n",
    "        ix = np.argsort(s)[::-1]\n",
    "        s = s[ix]\n",
    "        Q = Q[:, ix]\n",
    "        normalSqrtEigValuesList.append(np.diag(np.sqrt(s)))\n",
    "        normalEigVectorslist.append(Q)\n",
    "\n",
    "    vecTensors = nprnd.randn(np.prod(sizes), numSamples)\n",
    "    vecTensors = np.real(kron_mvprod(normalSqrtEigValuesList, vecTensors))\n",
    "    vecTensors = np.real(kron_mvprod(normalEigVectorslist, vecTensors))\n",
    "    \n",
    "    tensorList = []\n",
    "    for i in range(numSamples):\n",
    "        tensorList.append(np.reshape(vecTensors[:,i], tuple(sizes), order = 'F'))\n",
    "        \n",
    "    return tensorList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "numSamples = 100\n",
    "tensorList = tensorNormal(covConst, numSamples)\n",
    "\n",
    "estmarginalCovs = estTensorsMarginalCov(tensorList)\n",
    "for i in range(len(covConst)):\n",
    "    print \"Error in covariance %d is %.2f %%\" \\\n",
    "    %(i,(sciLA.norm(estmarginalCovs[i]-covConst[i], 'fro')/sciLA.norm(covConst[i], 'fro'))**2*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
